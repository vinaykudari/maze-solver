{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "032ceb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "from cnn import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "612dd503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_CNN(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\n",
    "            'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "        self.cnn = CNN(\n",
    "            img_dim=1 if env.greyscale else 3,\n",
    "            w=env.img_size[0],\n",
    "            h=env.img_size[1],\n",
    "            input_dim=np.prod(env.img_size),\n",
    "            output_dim=256,\n",
    "        )\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state.to(self.device)\n",
    "        output = self.cnn(state)\n",
    "        output = self.fc1(output)\n",
    "        output = F.relu(output)\n",
    "        state_val = self.fc2(output)\n",
    "        \n",
    "        return state_val\n",
    "        \n",
    "class Actor_CNN(nn.Module):\n",
    "    def __init__(self, env, action_dim):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\n",
    "            'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "        self.cnn = CNN(\n",
    "            img_dim=1 if env.greyscale else 3,\n",
    "            w=env.img_size[0],\n",
    "            h=env.img_size[1],\n",
    "            input_dim=np.prod(env.img_size),\n",
    "            output_dim=256,\n",
    "        )\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state.to(self.device)\n",
    "        output = self.cnn(state)\n",
    "        output = self.fc1(output)\n",
    "        output = F.relu(output)\n",
    "        policy = F.softmax(self.fc2(output), dim=-1)\n",
    "\n",
    "        return Categorical(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c8e6154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import FloatTensor as FT, tensor as T\n",
    "\n",
    "class A2C_CNN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        actor,\n",
    "        critic,\n",
    "        n_actns, \n",
    "        actor_optmz, \n",
    "        critic_optmz,\n",
    "        mdl_pth='../models/a2c',\n",
    "        log_freq=100,\n",
    "        hyprprms={},\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.n_actns = n_actns\n",
    "        self.actor_optmz = actor_optmz\n",
    "        self.critic_optmz = critic_optmz\n",
    "        self.log_freq = log_freq\n",
    "        self.mdl_pth = mdl_pth\n",
    "        self.hyprprms = hyprprms\n",
    "        self.gamma = self.hyprprms.get('gamma', 0.95),\n",
    "        self.step_sz = self.hyprprms.get('step_sz', 0.001)\n",
    "        self.eval_ep = self.hyprprms.get('eval_ep', 50)\n",
    "        self.logs = defaultdict(\n",
    "            lambda: {\n",
    "                'reward': 0,\n",
    "                'cum_reward': 0,\n",
    "            },\n",
    "        )\n",
    "        self.eval_logs = defaultdict(\n",
    "            lambda: {\n",
    "                'reward': 0,\n",
    "                'cum_reward': 0,\n",
    "            },\n",
    "        )\n",
    "        \n",
    "    @staticmethod\n",
    "    def _normalise(arr):\n",
    "        mean = np.mean(arr)\n",
    "        std = np.std(arr)\n",
    "        arr -= mean\n",
    "        arr /= (std + 1e-5)\n",
    "        return arr\n",
    "        \n",
    "        \n",
    "    def _get_returns(self, trmnl_state_val, rewards, gamma=1, normalise=True):\n",
    "        R = trmnl_state_val\n",
    "        returns = []\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = rewards[i] + gamma * R \n",
    "            returns.append(R)\n",
    "    \n",
    "        returns = returns[::-1]\n",
    "        if normalise:\n",
    "            returns = self._normalise(returns)\n",
    "            \n",
    "        return FT(returns)\n",
    "    \n",
    "    def _get_action(self, policy):\n",
    "        actn = T(policy.sample().item())\n",
    "        actn_log_prob = policy.log_prob(actn).unsqueeze(0)\n",
    "        return actn, actn_log_prob\n",
    "        \n",
    "    def train(self):\n",
    "        exp = []\n",
    "        state = self.env.reset()\n",
    "        ep_ended = False\n",
    "        ep_reward = 0\n",
    "        state = FT(state)\n",
    "        \n",
    "        while not ep_ended:\n",
    "            policy = self.actor(state)\n",
    "            actn, actn_log_prob = self._get_action(policy)\n",
    "            state_val = self.critic(state)\n",
    "                \n",
    "            _, reward, done, nxt_state, ep_ended = self.env.step(action=actn.item())\n",
    "            nxt_state = FT(nxt_state)\n",
    "            exp.append((nxt_state, state_val, T([reward]), actn_log_prob))\n",
    "            ep_reward += reward\n",
    "            \n",
    "            state = nxt_state\n",
    "            \n",
    "        states, state_vals, rewards, actn_log_probs = zip(*exp)\n",
    "        actn_log_probs = torch.cat(actn_log_probs)\n",
    "        state_vals = torch.cat(state_vals)\n",
    "        trmnl_state_val = self.critic(state).item()\n",
    "        returns = self._get_returns(trmnl_state_val, rewards).detach()\n",
    "        \n",
    "        \n",
    "        adv = returns - state_vals\n",
    "        actn_log_probs = actn_log_probs\n",
    "        actor_loss = (-1.0 * actn_log_probs * adv.detach()).mean()\n",
    "        critic_loss = adv.pow(2).mean()\n",
    "        net_loss = (actor_loss + critic_loss).mean()\n",
    "        \n",
    "        self.actor_optmz.zero_grad()\n",
    "        self.critic_optmz.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        self.actor_optmz.step()\n",
    "        self.critic_optmz.step()\n",
    "        \n",
    "        return net_loss, ep_reward\n",
    "    \n",
    "    def run(self, ep=1000):\n",
    "        for ep_no in range(ep):\n",
    "            ep_loss, ep_reward = self.train()\n",
    "            \n",
    "            self.logs[ep_no]['reward'] = ep_reward\n",
    "            if ep_no > 0:\n",
    "                self.logs[ep_no]['cum_reward'] += \\\n",
    "                self.logs[ep_no-1]['cum_reward']\n",
    "            \n",
    "            if ep_no % self.log_freq == 0:\n",
    "                print(f'Episode: {ep_no}, Loss: {ep_loss}, Avg. Reward: {ep_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba0a103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
